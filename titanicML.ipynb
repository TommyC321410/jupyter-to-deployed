{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build, Train, Deploy Titanic Data with AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Specific Imports and Setup\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='YOUR-S3-BUCKET' # Replace with your s3 bucket name\n",
    "prefix = 'linear-svc' # Used as part of the path in the bucket where you store data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket) # The URL to access the bucket\n",
    "\n",
    "raw_titanic_data = 's3://{}/{}'.format(bucket, 'rawTitanic.csv')\n",
    "\n",
    "print(raw_titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(raw_titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look over what data we have and a little bit about how it is structured. The 'info' function does a good job at showing what fields have null values, and we can learn about the different data types of our individual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head function also allows us to see the first 'n' amount of rows. This is great for diving a little deeper into what our dataset contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm that we are going to use, can't handle text, so let's map the 'Sex' column to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = {\"male\": 0, \"female\": 1}\n",
    "titanic['Sex'] = titanic['Sex'].map(sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an option with the name column. We can simply remove the column, or we can try to extract some value from it. Every person has a title in our dataset, so let's create a new column that is a numerical representation of what their title was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Rare\": 4}\n",
    "titanic['Title'] = titanic.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# replace titles with a more common title or as Rare\n",
    "titanic['Title'] = titanic['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                        'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Master'], 'Rare')\n",
    "titanic['Title'] = titanic['Title'].replace('Mlle', 'Miss')\n",
    "titanic['Title'] = titanic['Title'].replace('Ms', 'Miss')\n",
    "titanic['Title'] = titanic['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "titanic['Title'] = titanic['Title'].map(titles)\n",
    "\n",
    "# filling NaN with 0\n",
    "titanic['Title'] = titanic['Title'].fillna(0)\n",
    "titanic = titanic.drop(['Name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have many of our different features numeric, we can run a correlation matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = titanic.corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap=cmap, vmax=.3, linewidths=0.5, fmt='.2f',ax=ax)\n",
    "ax.set_ylim(8, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up our dataset. For our quick analysis, let's remove the columns or features that had a low correlation with our survived column. Let's also remove a few other features that we aren't going to try to parse to derive additional value. BUT! You absolutely could or would in a real situation. We just aren't going to for the nature of our quick demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.drop('PassengerId', 1)\n",
    "titanic = titanic.drop('Ticket', 1)\n",
    "titanic = titanic.drop('Cabin', 1)\n",
    "titanic = titanic.drop('Embarked', 1)\n",
    "titanic = titanic.drop('Age', 1)\n",
    "titanic = titanic.drop('SibSp', 1)\n",
    "titanic = titanic.drop('Parch', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at our data, we have a much more simple data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is looking good, but I think I may have spotted an issue. Lets look into the distribution of our 'fare' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fare feature has a standard deviation of 49! That is a pretty wide spread, considering that 75% of the data is below 31. So this tells us that we should probably use binning, to bin the fares into something that won't be as skewed by large values.\n",
    "\n",
    "Binning can be used when we have some extreme values, that are still in the expected range. These extreme values can influence the trained model greater than we want them to. We could also look into Normalization or Standarization, but binning is a great and simple approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.loc[ titanic['Fare'] <= 7.91, 'Fare'] = 0\n",
    "titanic.loc[(titanic['Fare'] > 7.91) & (titanic['Fare'] <= 14.454), 'Fare'] = 1\n",
    "titanic.loc[(titanic['Fare'] > 14.454) & (titanic['Fare'] <= 31), 'Fare']   = 2\n",
    "titanic.loc[(titanic['Fare'] > 31) & (titanic['Fare'] <= 99), 'Fare']   = 3\n",
    "titanic.loc[(titanic['Fare'] > 99) & (titanic['Fare'] <= 250), 'Fare']   = 4\n",
    "titanic.loc[ titanic['Fare'] > 250, 'Fare'] = 5\n",
    "titanic['Fare'] = titanic['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Our data is looking pretty good! But I think I see one last issue that we should take care of.\n",
    "\n",
    "We are going to use 'one hot encoding' for our last step. The issue that I see is with the Title column. Does it logically make sense for that feature to have order preserved? In other words, is a title of miss > mr? or master < dr?\n",
    "\n",
    "If we look at fare, for example, a fare of 5 should denote that it is greater than 4, so we want to keep it in the same column to denote that order matters. But because title SHOULD NOT have that same ordinal relationship, we can split that column up to remove that bias from our training model. To do that, we use a technique called 'one hot encoding' that splits the categories of that column (1, 2, 3, and 4) and gives them each their own column that has a binary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.get_dummies(titanic, prefix=['Title'], columns=['Title'])\n",
    "titanic.rename(columns={'Title_1': 'isMr', 'Title_2': 'isMiss', 'Title_3': 'isMrs', 'Title_4': 'isRareTitle'}, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data cleaned and ready, we are going to split our data into a 2/3, 1/3 split of training vs testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic.drop('Survived', 1)\n",
    "labels = titanic['Survived']\n",
    "\n",
    "train, test, train_labels, test_labels = train_test_split(features,\n",
    "                                                          labels,\n",
    "                                                          test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sagemaker needs the data to be in S3, so we are going to now need to move our split datasets into S3 so that we can do further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "test_csv_buffer = StringIO()\n",
    "train_csv_buffer = StringIO()\n",
    "pd.concat([test_labels, test], axis=1).to_csv(test_csv_buffer, header=True, index=False)\n",
    "pd.concat([train_labels, train], axis=1).to_csv(train_csv_buffer, header=True, index=False)\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, prefix + '/train.csv').put(Body=train_csv_buffer.getvalue())\n",
    "s3_resource.Object(bucket, prefix + '/validation.csv').put(Body=test_csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train.csv')\n",
    "\n",
    "validation_data = 's3://{}/{}/{}'.format(bucket, prefix, 'validation.csv')\n",
    "\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, prefix, 'xgboost_model_sdk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our xgboost algorithm, we need to fetch a container that contains that algorithm for us to use in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the container, now we can create the Estimator, set the hyperparameters, set where the data is coming from, and finally train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.m4.xlarge',\n",
    "                                         train_volume_size = 5,\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sagemaker.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.set_hyperparameters(max_depth = 5,\n",
    "                              eta = .2,\n",
    "                              gamma = 4,\n",
    "                              min_child_weight = 6,\n",
    "                              silent = 0,\n",
    "                              objective = 'multi:softmax',\n",
    "                              num_class = 2,\n",
    "                              num_round = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(train_data, content_type='text/csv')\n",
    "valid_channel = sagemaker.session.s3_input(validation_data, content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': valid_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are ready, we can train the model with the 'fit' method. The actual training time can vary, but this is what is actually building out your model and model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(inputs=data_channels,  logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Endpoint\n",
    "\n",
    "And with our model created and our model artifacts in S3, we can deploy our model. The Sagemaker SDK makes this incredibly easy for us. Sagemaker will create the model, endpoint configuration, as well as the endpoint, which are all hosted within Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                                instance_type='ml.t2.medium',\n",
    "                                endpoint_name='titanic-survived-predictor'\n",
    "                                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
